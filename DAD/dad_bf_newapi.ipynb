{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use torch backend\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/bayesflow/lib/python3.11/site-packages/keras/src/backend/torch/numpy.py:870: UserWarning: The operator 'aten::logspace.out' is not currently supported on the MPS backend and will fall back to run on the CPU. This may have performance implications. (Triggered internally at /Users/runner/miniforge3/conda-bld/libtorch_1715184405838/work/aten/src/ATen/mps/MPSFallback.mm:13.)\n",
      "  torch.logspace(\n"
     ]
    }
   ],
   "source": [
    "# setup\n",
    "import os\n",
    "os.environ[\"KERAS_BACKEND\"] = \"torch\"\n",
    "os.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"1\"\n",
    "\n",
    "\n",
    "import keras\n",
    "\n",
    "if keras.backend.backend() == \"torch\":\n",
    "    import torch\n",
    "    print(\"Use torch backend\")\n",
    "    torch.autograd.set_grad_enabled(False)\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "import bayesflow as bf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bayesflow.simulators.simulator import Simulator\n",
    "# from bayesflow.types import Shape, Tensor\n",
    "from torch import Tensor\n",
    "from torch.distributions import Distribution\n",
    "import torch.nn as nn\n",
    "from typing import Callable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyGenericSimulator(Simulator):\n",
    "    def __init__(self, context_sampler: Callable, prior_sampler: Callable, tau_sampler: Callable, design_generator: nn.Module, simulator_var: dict):\n",
    "        self.context_sampler = context_sampler\n",
    "        self.prior_sampler = prior_sampler\n",
    "        self.tau_sampler = tau_sampler\n",
    "        self.design_generator = design_generator\n",
    "        self.simulator_var = simulator_var\n",
    "\n",
    "    def sample(self, batch_size: torch.Size, context: Tensor = None, params : Tensor = None, tau : int = None, **kwargs) -> dict[str, Tensor]:\n",
    "\n",
    "        if context is None and params is None and tau is None:\n",
    "            context = self.context_sampler(batch_size)\n",
    "            params = self.prior_sampler.sample(context)\n",
    "            tau = self.tau_sampler()\n",
    "        \n",
    "        designs = []\n",
    "        outcomes = []\n",
    "\n",
    "        for t in range(tau):\n",
    "            xi = self.design_generator(batch_size)\n",
    "\n",
    "            # if params.shape[0] != xi.shape[0]: # for initial design\n",
    "            #     xi = xi.repeat(params.shape[0], 1)\n",
    "\n",
    "            y = self.outcome_simulator(params=params, xi=xi, simulator_var = self.simulator_var)\n",
    "\n",
    "            designs.append(xi)\n",
    "            outcomes.append(y)\n",
    "\n",
    "        designs = torch.stack(designs, dim=1).unsqueeze(-1)  #  [B, tau, 1]\n",
    "        outcomes = torch.stack(outcomes, dim=1) # [B, tau, 1]\n",
    "        n_obs = torch.sqrt(tau).repeat(batch_size).unsqueeze(1) # [B, 1]\n",
    "\n",
    "        out = {\"context\": context, \"params\": params, \"n_obs\": n_obs, \"designs\": designs, \"outcomes\": outcomes}\n",
    "\n",
    "        return out\n",
    "    \n",
    "    def outcome_simulator(self, params: Tensor, xi: Tensor) -> Tensor:\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomDesign(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, batch_size: torch.Size, designs: [Tensor] = None, outcomes: [Tensor] = None) -> Tensor:\n",
    "        return torch.rand(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LikelihoodBasedModel(MyGenericSimulator):\n",
    "    def __init__(self, context_sampler, prior_sampler, tau_sampler, design_generator, simulator_var) -> None:\n",
    "        super().__init__(context_sampler, prior_sampler, tau_sampler, design_generator, simulator_var)\n",
    "\n",
    "    def outcome_likelihood(self, params: Tensor, xi: Tensor) -> Distribution:\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def outcome_simulator(self, params: Tensor, xi: Tensor) -> Tensor:\n",
    "        return self.outcome_likelihood(params, xi, self.simulator_var).sample()\n",
    "    \n",
    "    def approximate_log_marginal_likelihood(self, context: Tensor, params: Tensor, xi: Tensor, outcomes: Tensor, log_approx_posterior: bf.networks) -> Tensor:\n",
    "\n",
    "        possible_masks = self.context_sampler.possible_masks\n",
    "        M = possible_masks.shape[0]\n",
    "\n",
    "        marginal_likelihood = []\n",
    "\n",
    "        for m in range(len(M)):\n",
    "            index = context[context == possible_masks[m]]\n",
    "            params_m = params[index]; xi_m = xi[index]; outcomes_m = outcomes[index]\n",
    "            first_term = self.outcome_likelihood(params_m, xi_m, self.simulator_var).log_prob(outcomes_m).sum()\n",
    "            second_term = self.prior_sampler.log_prob(params_m).sum()\n",
    "            third_term = log_approx_posterior.log_prob(params_m, outcomes_m).sum()\n",
    "\n",
    "            marginal_likelihood_m = torch.exp(first_term + second_term - third_term)\n",
    "            marginal_likelihood.append(marginal_likelihood_m)\n",
    "\n",
    "        marginal_likelihood = torch.stack(marginal_likelihood, dim = 0) / torch.stack(marginal_likelihood, dim = 0).sum()\n",
    "        \n",
    "\n",
    "        return marginal_likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolynomialRegression(LikelihoodBasedModel):\n",
    "    def __init__(self, context_sampler, prior_sampler, tau_sampler, design_generator, simulator_var) -> None:\n",
    "        super().__init__(context_sampler, prior_sampler, tau_sampler, design_generator, simulator_var)\n",
    "\n",
    "    def outcome_likelihood(self, params: Tensor, xi: Tensor, simulator_var: dict) -> Distribution:\n",
    "\n",
    "        xi_powers = torch.stack([torch.ones_like(xi), xi, xi ** 2, xi ** 3], dim=1)\n",
    "        mean = torch.sum(params * xi_powers, dim=-1, keepdim=True)\n",
    "        sigma = simulator_var[\"sigma\"]\n",
    "        return torch.distributions.Normal(mean, sigma)\n",
    "    \n",
    "    def analytical_log_marginal_likelihood(outcomes, params: Tensor, param_mask: Tensor) -> Tensor:\n",
    "        raise NotImplementedError # TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParameterMask:\n",
    "    def __init__(self, num_parameters: int = 4, possible_masks: Tensor = None) -> None:\n",
    "        default_mask = torch.tril(torch.ones((num_parameters, num_parameters)))\n",
    "        self.num_parameters = num_parameters\n",
    "        self.possible_masks = torch.tensor(possible_masks, dtype=torch.float32) if possible_masks is not None else default_mask\n",
    "\n",
    "    def __call__(self, batch_shape: torch.Size) -> Tensor:\n",
    "        index_samples = torch.randint(0, self.possible_masks.shape[0], batch_shape, dtype=torch.long)\n",
    "        out_mask = self.possible_masks[index_samples]\n",
    "\n",
    "        return out_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Prior():\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "    \n",
    "    def dist_list(self, param_mask: Tensor) -> [Distribution]:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def sample(self, param_mask: Tensor) -> Tensor:\n",
    "        return torch.stack([dist.sample() for dist in self.dist_list(param_mask)], dim = 0)\n",
    "\n",
    "    def log_prob(self, params: Tensor, param_mask: Tensor) -> Tensor:\n",
    "        return torch.stack([dist.log_prob(param) for dist, param in zip(self.dist_list(param_mask), params)], dim = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PriorPolynomialReg(Prior):\n",
    "    def __init__(self, delta: Tensor = Tensor([0.1])) -> None:\n",
    "        super().__init__()\n",
    "        self.delta = delta\n",
    "\n",
    "    def dist_list(self, param_mask: Tensor) -> [Distribution]:\n",
    "        super().__init__()\n",
    "        \n",
    "        self.param_mask = param_mask\n",
    "\n",
    "        default = Tensor([[0, self.delta]])\n",
    "        param_mask_unsq = param_mask.unsqueeze(-1)\n",
    "\n",
    "        prior_0 = torch.where(param_mask_unsq[:, 0] == 1, Tensor([5, 2]), default)\n",
    "        prior_1 = torch.where(param_mask_unsq[:, 1] == 1, Tensor([3, 1]), default)\n",
    "        prior_2 = torch.where(param_mask_unsq[:, 2] == 1, Tensor([0, 0.8]), default)\n",
    "        prior_3 = torch.where(param_mask_unsq[:, 3] == 1, Tensor([0, 0.5]), default)\n",
    "\n",
    "        hyper_params = torch.stack([prior_0, prior_1, prior_2, prior_3], dim=1)\n",
    "\n",
    "        mean_s = hyper_params[:, :, 0]\n",
    "        sigma_s = hyper_params[:, :, 1]\n",
    "    \n",
    "        dist_list = [torch.distributions.MultivariateNormal(mean, scale_tril=torch.diag(sigma)) for mean, sigma in zip(mean_s, sigma_s)]\n",
    "\n",
    "        return dist_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "B = 64\n",
    "batch_size = torch.Size([B])\n",
    "\n",
    "param_mask_generator = ParameterMask()\n",
    "param_mask = param_mask_generator(batch_size)\n",
    "polynomial_reg = PriorPolynomialReg()\n",
    "params = polynomial_reg.sample(param_mask)\n",
    "likelihood = polynomial_reg.log_prob(params, param_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of parameter mask torch.Size([64, 4])\n",
      "Shape of parameters torch.Size([64, 4])\n",
      "Shape of likelihood torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "print(f\"Shape of parameter mask {param_mask.shape}\") # [B, model_dim]\n",
    "print(f\"Shape of parameters {params.shape}\") # [B, param_dim]\n",
    "print(f\"Shape of likelihood {likelihood.shape}\") # [B]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter_mask = ParameterMask()\n",
    "random_design_generator = RandomDesign()\n",
    "prior = PriorPolynomialReg()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "class random_num_obs():\n",
    "    def __init__(self, min_obs : int = 1, max_obs : int = T) -> Tensor:\n",
    "        self.min_obs = min_obs\n",
    "        self.max_obs = max_obs # T\n",
    "\n",
    "    def __call__(self):\n",
    "        return torch.randint(self.min_obs, self.max_obs + 1, (1,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = 10\n",
    "random_num_obs = random_num_obs(min_obs = 1, max_obs = T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "polynomial_reg = PolynomialRegression(context_sampler = parameter_mask,\n",
    "                                      prior_sampler = prior,\n",
    "                                      tau_sampler = random_num_obs,\n",
    "                                      design_generator = random_design_generator,\n",
    "                                      simulator_var = {\"sigma\": 1.0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = polynomial_reg.sample(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['context', 'params', 'n_obs', 'designs', 'outcomes'])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataSet(keras.utils.PyDataset):\n",
    "    def __init__(self, batch_size: torch.Size, stage: int, initial_generative_model: MyGenericSimulator, design_network: nn.Module = None):\n",
    "        super().__init__()\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.stage = stage # stage 1,2,3\n",
    "        self.initial_generative_model = initial_generative_model\n",
    "        self.design_network = design_network\n",
    "\n",
    "    def __getitem__(self, item:int) -> dict[str, Tensor]:\n",
    "        if self.stage == 1:\n",
    "\n",
    "            data = self.initial_generative_model.sample(self.batch_size)\n",
    "            return data\n",
    "\n",
    "        if self.stage == 2:\n",
    "            second_generative_model = 1\n",
    "            data = self.second_generative_model.sampel(self.batch_size, )\n",
    "            return data\n",
    "\n",
    "        if self.stage == 3:\n",
    "            ...\n",
    "            return data\n",
    "    \n",
    "    @property\n",
    "    def num_batches(self):\n",
    "        # infinite dataset\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = MyDataSet(batch_size = batch_size, stage = 1, initial_generative_model = polynomial_reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_network = bf.networks.CouplingFlow(depth = 8, subnet_kwargs=dict(kernel_regularizer=None, dropout_prob = False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_network = bf.networks.DeepSet(summary_dim = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "approximator = bf.Approximator(\n",
    "    inference_network = inference_network,\n",
    "    summary_network = summary_network,\n",
    "    inference_variables = [\"params\"],\n",
    "    inference_conditions = [\"context\", \"n_obs\"],\n",
    "    summary_variables = [\"outcomes\", \"designs\"]\n",
    ")\n",
    "\n",
    "approximator.compile(optimizer=\"AdamW\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 2s/step - inference/loss: 15.8874 - loss: 15.8874 - summary/loss: 0.0000e+00\n",
      "Epoch 2/10\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 2s/step - inference/loss: 7.0853 - loss: 7.0853 - summary/loss: 0.0000e+00\n",
      "Epoch 3/10\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 2s/step - inference/loss: 4.9676 - loss: 4.9676 - summary/loss: 0.0000e+00\n",
      "Epoch 4/10\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 2s/step - inference/loss: 4.0515 - loss: 4.0515 - summary/loss: 0.0000e+00\n",
      "Epoch 5/10\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 2s/step - inference/loss: 3.1545 - loss: 3.1545 - summary/loss: 0.0000e+00\n",
      "Epoch 6/10\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 2s/step - inference/loss: 3.2678 - loss: 3.2678 - summary/loss: 0.0000e+00\n",
      "Epoch 7/10\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 2s/step - inference/loss: 3.2170 - loss: 3.2170 - summary/loss: 0.0000e+00\n",
      "Epoch 8/10\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 2s/step - inference/loss: 2.4520 - loss: 2.4520 - summary/loss: 0.0000e+00\n",
      "Epoch 9/10\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 2s/step - inference/loss: 2.4993 - loss: 2.4993 - summary/loss: 0.0000e+00\n",
      "Epoch 10/10\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 2s/step - inference/loss: 1.8325 - loss: 1.8325 - summary/loss: 0.0000e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x155f879d0>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "approximator.fit(dataset, epochs=10, steps_per_epoch=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0.], device='mps:0')"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "approximator.configurator.configure_summary_variables({\"outcomes\": Tensor([0]), \"designs\": Tensor([0])})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#outcomes = torch.zeros((1, 2))\n",
    "#approximator.sample(batch_size, data = {\"outcomes\": Tensor([0]), \"designs\": Tensor([0])})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO at last\n",
    "\n",
    "class InferenceDesignApproximator:\n",
    "    def __init__(self, hyperparameters: dict, bf_settings: dict, design_settings: dict):\n",
    "\n",
    "        self.summary_network = ...\n",
    "\n",
    "        # Dataset object: online dataset https://github.com/stefanradev93/BayesFlow/blob/streamlined-backend/bayesflow/datasets/online_dataset.py\n",
    "        self.dataset: MyDataSet\n",
    "\n",
    "        # BayesFlow approximator is encapsuled\n",
    "        self.bf_approximator = bf.approximators.Approximator(..., summary_network, **bf_settings)\n",
    "        \n",
    "        # Design network object is encapsuled\n",
    "        self.design_net = DesignNetwork(..., summary_network, **design_settings)\n",
    "\n",
    "        # Hyperparameters: weight terms to balance losses, etc.\n",
    "        self.hyperparameters = hyperparameters\n",
    "\n",
    "    def train(self, dataset):\n",
    "        # Stage 1: Train Bayesflow, use random design\n",
    "        self.bf_approximator.train(dataset)\n",
    "        self.dataset.stage = 2\n",
    "\n",
    "        # Stage 2: Fix BayesFlow, train design network\n",
    "        self.bf_approximator.freeze_weights() # implement this\n",
    "        self.design_approximator.train(dataset)\n",
    "        self.dataset.stage = 3\n",
    "\n",
    "        # Stage 3: Joint training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepAdaptiveDesign(nn.Module):\n",
    "  def __init__(\n",
    "      self,\n",
    "      encoder_net: nn.Module | bf.networks.CouplingFlow,\n",
    "      decoder_net: nn.Module,\n",
    "      design_shape: torch.Size\n",
    "    ) -> None:\n",
    "    super().__init__()\n",
    "    self.design_shape = design_shape\n",
    "    # initialise first design with random normal\n",
    "    self.register_parameter(\n",
    "        \"initial_design\",\n",
    "        nn.Parameter(0.1 * torch.ones(design_shape, dtype=torch.float32))\n",
    "    )\n",
    "    self.encoder_net = encoder_net\n",
    "    self.decoder_net = decoder_net\n",
    "\n",
    "  def forward(self, designs=list[Tensor], outcomes=list[Tensor]) -> Tensor:\n",
    "    if len(outcomes) == 0:\n",
    "      return self.initial_design\n",
    "    else:\n",
    "      # embed design-outcome pairs\n",
    "      embeddings = torch.cat([self.encoder_net(xi, y) for (xi, y) in zip(designs, outcomes)]) # TODO aggregate [, dim]\n",
    "      # get next design\n",
    "      next_design = self.decoder_net(embeddings)\n",
    "    return next_design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmitterNetwork(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim,\n",
    "        hidden_dim,\n",
    "        output_dim,\n",
    "        n_hidden_layers=2,\n",
    "        activation=nn.Softplus,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.activation_layer = activation()\n",
    "        self.input_layer = nn.Linear(input_dim, hidden_dim)\n",
    "        if n_hidden_layers > 1:\n",
    "            self.middle = nn.Sequential(\n",
    "                *[\n",
    "                    nn.Sequential(nn.Linear(hidden_dim, hidden_dim), activation())\n",
    "                    for _ in range(n_hidden_layers - 1)\n",
    "                ]\n",
    "            )\n",
    "        else:\n",
    "            self.middle = nn.Identity()\n",
    "        self.output_layer = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, r):\n",
    "        x = self.input_layer(r)\n",
    "        x = self.activation_layer(x)\n",
    "        x = self.middle(x)\n",
    "        x = self.output_layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MutualInformation(nn.Module):\n",
    "  def __init__(self, joint_model, batch_size: int) -> None:\n",
    "    super().__init__()\n",
    "    self.joint_model = joint_model\n",
    "    self.batch_size = batch_size\n",
    "\n",
    "  def forward(self) -> Tensor:\n",
    "    raise NotImplemented\n",
    "\n",
    "  def estimate(num_eval_samples) -> float:\n",
    "    raise NotImplemented\n",
    "\n",
    "class NestedMonteCarlo(MutualInformation):\n",
    "  def __init__(\n",
    "      self,\n",
    "      joint_model: LikelihoodBasedModel,\n",
    "      amortized_posterior: bf.networks,\n",
    "      batch_size: torch.Size,\n",
    "      num_negative_samples: int,\n",
    "      lower_bound: bool = True\n",
    "      ) -> None:\n",
    "    super().__init__(joint_model=joint_model, batch_size=batch_size)\n",
    "    self.num_negative_samples = num_negative_samples # L\n",
    "    self.lower_bound = lower_bound\n",
    "\n",
    "  def forward(self) -> Tensor:\n",
    "\n",
    "    # simulate history\n",
    "    context_h, params_h, tau, xi_h, y_h = self.joint_model.sample(self.batch_size).values()\n",
    "\n",
    "    post_model_prob = self.joint_model.approximate_log_marginal_likelihood(context_h, params_h, xi_h, y_h)\n",
    "\n",
    "    context = np.random.choice(self.joint_model.context_sampler.possible_masks, \n",
    "                               size = self.batch_size, p = post_model_prob)\n",
    "    \n",
    "    prior_samples_primary = self.amortized_posterior.sample(context) # TODO\n",
    "\n",
    "    n_obs = self.joint_model.tau_sampler.max_obs - tau\n",
    "\n",
    "    _, _, _, designs, outcomes = self.joint_model.sample(self.batch_size, context = context, params = params, n_obs = n_obs)\n",
    "\n",
    "    # we can resuse negative samples\n",
    "    prior_samples_negative = self.amortized_posterior.sample(\n",
    "        torch.Size([self.num_negative_samples])\n",
    "    ).unsqueeze(1) # [num_neg_samples, ...] -> [num_neg_samples, 1, ...]\n",
    "\n",
    "    # evaluate the logprob of outcomes under the primary:\n",
    "    logprob_primary = torch.stack([\n",
    "        self.joint_model.outcome_likelihood(\n",
    "            prior_samples_primary, xi\n",
    "        ).log_prob(y) for (xi, y) in zip(designs, outcomes)\n",
    "    ], dim=0).sum(0) # [T, B] -> [B]\n",
    "\n",
    "    # evaluate the logprob of outcomes under the contrastive parameter samples:\n",
    "    logprob_negative = torch.stack([\n",
    "        self.joint_model.outcome_likelihood(\n",
    "            prior_samples_negative, xi.unsqueeze(0) # add dim for <num_neg_samples>\n",
    "        ).log_prob(y.unsqueeze(0)) for (xi, y) in zip(designs, outcomes)\n",
    "    ], dim=0).sum(0) # [T, num_neg_samples, B] -> [num_neg_samples, B]\n",
    "\n",
    "    print(\"nagative param\", prior_samples_negative.shape)\n",
    "    print(\"one design\", designs[0].unsqueeze(0).shape)\n",
    "    print(\"one outcomes\", outcomes[0].unsqueeze(0).shape)\n",
    "\n",
    "    # if lower bound, log_prob primary should be added to the denominator\n",
    "    if self.lower_bound:\n",
    "      # concat primary and negative to get [negative_b + 1, B] for the logsumexp\n",
    "      logprob_negative = torch.cat([\n",
    "          logprob_negative, logprob_primary.unsqueeze(0)]\n",
    "      ) # [num_neg_samples + 1, B]\n",
    "      to_logmeanexp = torch.log(self.num_negative_samples + 1)\n",
    "    else:\n",
    "      to_logmeanexp = torch.log(self.num_negative_samples)\n",
    "\n",
    "    log_denom = torch.logsumexp(logprob_negative, dim=0) - to_logmeanexp # [B]\n",
    "    mi = (logprob_primary - log_denom).mean(0) # [B] -> scalar\n",
    "    return -mi\n",
    "\n",
    "  def estimate(self) -> float:\n",
    "    with torch.no_grad():\n",
    "      loss = self.forward()\n",
    "    return -loss.item()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bayesflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
