{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use torch backend\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/bayesflow/lib/python3.11/site-packages/keras/src/backend/torch/numpy.py:870: UserWarning: The operator 'aten::logspace.out' is not currently supported on the MPS backend and will fall back to run on the CPU. This may have performance implications. (Triggered internally at /Users/runner/miniforge3/conda-bld/libtorch_1715184405838/work/aten/src/ATen/mps/MPSFallback.mm:13.)\n",
      "  torch.logspace(\n"
     ]
    }
   ],
   "source": [
    "# setup\n",
    "import os\n",
    "os.environ[\"KERAS_BACKEND\"] = \"torch\"\n",
    "os.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"1\"\n",
    "\n",
    "\n",
    "import keras\n",
    "\n",
    "if keras.backend.backend() == \"torch\":\n",
    "    import torch\n",
    "    print(\"Use torch backend\")\n",
    "    torch.autograd.set_grad_enabled(False)\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "import bayesflow as bf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bayesflow.simulators.simulator import Simulator\n",
    "# from bayesflow.types import Shape, Tensor\n",
    "from torch import Tensor\n",
    "from torch.distributions import Distribution\n",
    "import torch.nn as nn\n",
    "from typing import Callable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyGenericSimulator(Simulator):\n",
    "    def __init__(self, context_sampler: Callable, prior_sampler: Callable, tau_sampler: Callable, design_generator: nn.Module, simulator_var: dict):\n",
    "        self.context_sampler = context_sampler\n",
    "        self.prior_sampler = prior_sampler\n",
    "        self.tau_sampler = tau_sampler\n",
    "        self.design_generator = design_generator\n",
    "        self.simulator_var = simulator_var\n",
    "\n",
    "    def sample(self, batch_size: torch.Size, context: Tensor = None, params : Tensor = None, tau : int = None, **kwargs) -> dict[str, Tensor]:\n",
    "\n",
    "        if context is None and params is None and tau is None:\n",
    "            context = self.context_sampler(batch_size)\n",
    "            params = self.prior_sampler.sample(context)\n",
    "            tau = self.tau_sampler()\n",
    "        \n",
    "        designs = []\n",
    "        outcomes = []\n",
    "\n",
    "        for t in range(tau):\n",
    "            xi = self.design_generator(batch_size)\n",
    "\n",
    "            # if params.shape[0] != xi.shape[0]: # for initial design\n",
    "            #     xi = xi.repeat(params.shape[0], 1)\n",
    "\n",
    "            y = self.outcome_simulator(params=params, xi=xi)\n",
    "\n",
    "            designs.append(xi)\n",
    "            outcomes.append(y)\n",
    "\n",
    "        designs = torch.stack(designs, dim=1).unsqueeze(-1)  #  [B, tau, 1]\n",
    "        outcomes = torch.stack(outcomes, dim=1) # [B, tau, 1]\n",
    "        n_obs = torch.sqrt(tau).repeat(batch_size).unsqueeze(1) # [B, 1]\n",
    "\n",
    "        out = {\"context\": context, \"params\": params, \"n_obs\": n_obs, \"designs\": designs, \"outcomes\": outcomes}\n",
    "\n",
    "        return out\n",
    "    \n",
    "    def outcome_simulator(self, params: Tensor, xi: Tensor) -> Tensor:\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomDesign(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, batch_size: torch.Size, designs: [Tensor] = None, outcomes: [Tensor] = None) -> Tensor:\n",
    "        return torch.rand(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LikelihoodBasedModel(MyGenericSimulator):\n",
    "    def __init__(self, context_sampler, prior_sampler, tau_sampler, design_generator, simulator_var) -> None:\n",
    "        super().__init__(context_sampler, prior_sampler, tau_sampler, design_generator, simulator_var)\n",
    "\n",
    "    def outcome_likelihood(self, params: Tensor, xi: Tensor) -> Distribution:\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def outcome_simulator(self, params: Tensor, xi: Tensor) -> Tensor:\n",
    "        return self.outcome_likelihood(params, xi, self.simulator_var).sample()\n",
    "    \n",
    "    def approximate_log_marginal_likelihood(self, context: Tensor, params: Tensor, xi: Tensor, outcomes: Tensor, log_approx_posterior: bf.networks) -> Tensor:\n",
    "\n",
    "        possible_masks = self.context_sampler.possible_masks\n",
    "        M = possible_masks.shape[0]\n",
    "\n",
    "        marginal_likelihood = []\n",
    "\n",
    "        for m in range(len(M)):\n",
    "            index = context[context == possible_masks[m]]\n",
    "            params_m = params[index]; xi_m = xi[index]; outcomes_m = outcomes[index]\n",
    "            first_term = self.outcome_likelihood(params_m, xi_m, self.simulator_var).log_prob(outcomes_m).sum()\n",
    "            second_term = self.prior_sampler.log_prob(params_m).sum()\n",
    "            third_term = log_approx_posterior.log_prob(params_m, outcomes_m).sum()\n",
    "\n",
    "            marginal_likelihood_m = torch.exp(first_term + second_term - third_term)\n",
    "            marginal_likelihood.append(marginal_likelihood_m)\n",
    "\n",
    "        marginal_likelihood = torch.stack(marginal_likelihood, dim = 0) / torch.stack(marginal_likelihood, dim = 0).sum()\n",
    "        \n",
    "\n",
    "        return marginal_likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolynomialRegression(LikelihoodBasedModel):\n",
    "    def __init__(self, context_sampler, prior_sampler, tau_sampler, design_generator, simulator_var) -> None:\n",
    "        super().__init__(context_sampler, prior_sampler, tau_sampler, design_generator, simulator_var)\n",
    "\n",
    "    def outcome_likelihood(self, params: Tensor, xi: Tensor, simulator_var: dict) -> Distribution:\n",
    "\n",
    "        xi_powers = torch.stack([torch.ones_like(xi), xi, xi ** 2, xi ** 3], dim=1)\n",
    "        mean = torch.sum(params * xi_powers, dim=-1, keepdim=True)\n",
    "        sigma = simulator_var[\"sigma\"]\n",
    "        return torch.distributions.Normal(mean, sigma)\n",
    "    \n",
    "    def analytical_log_marginal_likelihood(outcomes, params: Tensor, param_mask: Tensor) -> Tensor:\n",
    "        raise NotImplementedError # TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParameterMask:\n",
    "    def __init__(self, num_parameters: int = 4, possible_masks: Tensor = None) -> None:\n",
    "        default_mask = torch.tril(torch.ones((num_parameters, num_parameters)))\n",
    "        self.num_parameters = num_parameters\n",
    "        self.possible_masks = torch.tensor(possible_masks, dtype=torch.float32) if possible_masks is not None else default_mask\n",
    "\n",
    "    def __call__(self, batch_shape: torch.Size) -> Tensor:\n",
    "        index_samples = torch.randint(0, self.possible_masks.shape[0], batch_shape, dtype=torch.long)\n",
    "        out_mask = self.possible_masks[index_samples]\n",
    "\n",
    "        return out_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Prior():\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "    \n",
    "    def dist_list(self, param_mask: Tensor) -> [Distribution]:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def sample(self, param_mask: Tensor) -> Tensor:\n",
    "        return torch.stack([dist.sample() for dist in self.dist_list(param_mask)], dim = 0)\n",
    "\n",
    "    def log_prob(self, params: Tensor, param_mask: Tensor) -> Tensor:\n",
    "        return torch.stack([dist.log_prob(param) for dist, param in zip(self.dist_list(param_mask), params)], dim = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PriorPolynomialReg(Prior):\n",
    "    def __init__(self, delta: Tensor = Tensor([0.1])) -> None:\n",
    "        super().__init__()\n",
    "        self.delta = delta\n",
    "\n",
    "    def dist_list(self, param_mask: Tensor) -> [Distribution]:\n",
    "        super().__init__()\n",
    "        \n",
    "        self.param_mask = param_mask\n",
    "\n",
    "        default = Tensor([[0, self.delta]])\n",
    "        param_mask_unsq = param_mask.unsqueeze(-1)\n",
    "\n",
    "        prior_0 = torch.where(param_mask_unsq[:, 0] == 1, Tensor([5, 2]), default)\n",
    "        prior_1 = torch.where(param_mask_unsq[:, 1] == 1, Tensor([3, 1]), default)\n",
    "        prior_2 = torch.where(param_mask_unsq[:, 2] == 1, Tensor([0, 0.8]), default)\n",
    "        prior_3 = torch.where(param_mask_unsq[:, 3] == 1, Tensor([0, 0.5]), default)\n",
    "\n",
    "        hyper_params = torch.stack([prior_0, prior_1, prior_2, prior_3], dim=1)\n",
    "\n",
    "        mean_s = hyper_params[:, :, 0]\n",
    "        sigma_s = hyper_params[:, :, 1]\n",
    "    \n",
    "        dist_list = [torch.distributions.MultivariateNormal(mean, scale_tril=torch.diag(sigma)) for mean, sigma in zip(mean_s, sigma_s)]\n",
    "\n",
    "        return dist_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "B = 64\n",
    "batch_size = torch.Size([B])\n",
    "\n",
    "param_mask_generator = ParameterMask()\n",
    "param_mask = param_mask_generator(batch_size)\n",
    "polynomial_reg = PriorPolynomialReg()\n",
    "params = polynomial_reg.sample(param_mask)\n",
    "likelihood = polynomial_reg.log_prob(params, param_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of parameter mask torch.Size([64, 4])\n",
      "Shape of parameters torch.Size([64, 4])\n",
      "Shape of likelihood torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "print(f\"Shape of parameter mask {param_mask.shape}\") # [B, model_dim]\n",
    "print(f\"Shape of parameters {params.shape}\") # [B, param_dim]\n",
    "print(f\"Shape of likelihood {likelihood.shape}\") # [B]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter_mask = ParameterMask()\n",
    "random_design_generator = RandomDesign()\n",
    "prior = PriorPolynomialReg()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class random_num_obs():\n",
    "    def __init__(self, min_obs : int, max_obs: int) -> Tensor:\n",
    "        self.min_obs = min_obs\n",
    "        self.max_obs = max_obs # T\n",
    "\n",
    "    def __call__(self):\n",
    "        return torch.randint(self.min_obs, self.max_obs + 1, (1,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = 10\n",
    "random_num_obs = random_num_obs(min_obs = 1, max_obs = T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "polynomial_reg = PolynomialRegression(context_sampler = parameter_mask,\n",
    "                                      prior_sampler = prior,\n",
    "                                      tau_sampler = random_num_obs,\n",
    "                                      design_generator = random_design_generator,\n",
    "                                      simulator_var = {\"sigma\": 1.0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = polynomial_reg.sample(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['context', 'params', 'n_obs', 'designs', 'outcomes'])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataSet(keras.utils.PyDataset):\n",
    "    def __init__(self, batch_size: torch.Size, stage: int, initial_generative_model: MyGenericSimulator, design_network: nn.Module = None):\n",
    "        super().__init__()\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.stage = stage # stage 1,2,3\n",
    "        self.initial_generative_model = initial_generative_model\n",
    "        self.design_network = design_network\n",
    "\n",
    "    def __getitem__(self, item:int) -> dict[str, Tensor]:\n",
    "        if self.stage == 1:\n",
    "\n",
    "            data = self.initial_generative_model.sample(self.batch_size)\n",
    "            return data\n",
    "\n",
    "        if self.stage == 2:\n",
    "            second_generative_model = 1\n",
    "            data = self.second_generative_model.sampel(self.batch_size, )\n",
    "            return data\n",
    "\n",
    "        if self.stage == 3:\n",
    "            ...\n",
    "            return data\n",
    "    \n",
    "    @property\n",
    "    def num_batches(self):\n",
    "        # infinite dataset\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = MyDataSet(batch_size = batch_size, stage = 1, initial_generative_model = polynomial_reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_network = bf.networks.CouplingFlow(depth = 8, subnet_kwargs=dict(kernel_regularizer=None, dropout_prob = False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_network = bf.networks.DeepSet(summary_dim = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "approximator = bf.Approximator(\n",
    "    inference_network = inference_network,\n",
    "    summary_network = summary_network,\n",
    "    inference_variables = [\"params\"],\n",
    "    inference_conditions = [\"context\", \"n_obs\"],\n",
    "    summary_variables = [\"outcomes\", \"designs\"]\n",
    ")\n",
    "\n",
    "approximator.compile(optimizer=\"AdamW\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 2s/step - inference/loss: 53.6691 - loss: 53.6691 - summary/loss: 0.0000e+00\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[45], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mapproximator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/thesis/BayesFlow_DAD/DAD/../bayesflow/approximators/base_approximator.py:176\u001b[0m, in \u001b[0;36mBaseApproximator.fit\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    173\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(dataset))\n\u001b[1;32m    174\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuild_from_data(data)\n\u001b[0;32m--> 176\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/bayesflow/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/bayesflow/lib/python3.11/site-packages/keras/src/backend/torch/trainer.py:254\u001b[0m, in \u001b[0;36mTorchTrainer.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, data \u001b[38;5;129;01min\u001b[39;00m epoch_iterator\u001b[38;5;241m.\u001b[39menumerate_epoch():\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;66;03m# Callbacks\u001b[39;00m\n\u001b[1;32m    252\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m--> 254\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    255\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pythonify_logs(logs)\n\u001b[1;32m    257\u001b[0m     \u001b[38;5;66;03m# Callbacks\u001b[39;00m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/bayesflow/lib/python3.11/site-packages/keras/src/backend/torch/trainer.py:117\u001b[0m, in \u001b[0;36mTorchTrainer.make_train_function.<locals>.one_step_on_data\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Runs a single training step on a batch of data.\"\"\"\u001b[39;00m\n\u001b[1;32m    116\u001b[0m data \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m--> 117\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/thesis/BayesFlow_DAD/DAD/../bayesflow/approximators/torch_approximator.py:22\u001b[0m, in \u001b[0;36mTorchApproximator.train_step\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Update weights\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 22\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgradients\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainable_weights\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_loss_tracker\u001b[38;5;241m.\u001b[39mupdate_state(loss)\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m metrics\n",
      "File \u001b[0;32m/opt/miniconda3/envs/bayesflow/lib/python3.11/site-packages/keras/src/optimizers/base_optimizer.py:351\u001b[0m, in \u001b[0;36mBaseOptimizer.apply\u001b[0;34m(self, grads, trainable_variables)\u001b[0m\n\u001b[1;32m    348\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_apply_weight_decay(trainable_variables)\n\u001b[1;32m    350\u001b[0m \u001b[38;5;66;03m# Apply gradient updates.\u001b[39;00m\n\u001b[0;32m--> 351\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backend_apply_gradients\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainable_variables\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[38;5;66;03m# Apply variable constraints after applying gradients.\u001b[39;00m\n\u001b[1;32m    353\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m variable \u001b[38;5;129;01min\u001b[39;00m trainable_variables:\n",
      "File \u001b[0;32m/opt/miniconda3/envs/bayesflow/lib/python3.11/site-packages/keras/src/optimizers/base_optimizer.py:405\u001b[0m, in \u001b[0;36mBaseOptimizer._backend_apply_gradients\u001b[0;34m(self, grads, trainable_variables)\u001b[0m\n\u001b[1;32m    396\u001b[0m     ops\u001b[38;5;241m.\u001b[39mcond(\n\u001b[1;32m    397\u001b[0m         is_update_step,\n\u001b[1;32m    398\u001b[0m         \u001b[38;5;28;01mlambda\u001b[39;00m: _update_step_fn(grads, trainable_variables),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    401\u001b[0m         ),\n\u001b[1;32m    402\u001b[0m     )\n\u001b[1;32m    403\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    404\u001b[0m     \u001b[38;5;66;03m# Run udpate step.\u001b[39;00m\n\u001b[0;32m--> 405\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backend_update_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    406\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainable_variables\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearning_rate\u001b[49m\n\u001b[1;32m    407\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    409\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_ema:\n\u001b[1;32m    410\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_model_variables_moving_average(\n\u001b[1;32m    411\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trainable_variables\n\u001b[1;32m    412\u001b[0m     )\n",
      "File \u001b[0;32m/opt/miniconda3/envs/bayesflow/lib/python3.11/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/bayesflow/lib/python3.11/site-packages/keras/src/backend/torch/optimizers/torch_parallel_optimizer.py:10\u001b[0m, in \u001b[0;36mTorchParallelOptimizer._backend_update_step\u001b[0;34m(self, grads, trainable_variables, learning_rate)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;129m@torch_utils\u001b[39m\u001b[38;5;241m.\u001b[39mno_grad\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_backend_update_step\u001b[39m(\u001b[38;5;28mself\u001b[39m, grads, trainable_variables, learning_rate):\n\u001b[0;32m---> 10\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parallel_update_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrainable_variables\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/bayesflow/lib/python3.11/site-packages/keras/src/backend/torch/optimizers/torch_adam.py:35\u001b[0m, in \u001b[0;36mAdam._parallel_update_step\u001b[0;34m(self, grads, variables, learning_rate)\u001b[0m\n\u001b[1;32m     26\u001b[0m m_list \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_momentums[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_variable_index(variable)]\u001b[38;5;241m.\u001b[39mvalue\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m variable \u001b[38;5;129;01min\u001b[39;00m keras_variables\n\u001b[1;32m     29\u001b[0m ]\n\u001b[1;32m     30\u001b[0m v_list \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_velocities[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_variable_index(variable)]\u001b[38;5;241m.\u001b[39mvalue\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m variable \u001b[38;5;129;01min\u001b[39;00m keras_variables\n\u001b[1;32m     33\u001b[0m ]\n\u001b[0;32m---> 35\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_foreach_mul_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mm_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbeta_1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m torch\u001b[38;5;241m.\u001b[39m_foreach_add_(m_list, grads, alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbeta_1)\n\u001b[1;32m     38\u001b[0m torch\u001b[38;5;241m.\u001b[39m_foreach_mul_(v_list, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbeta_2)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "approximator.fit(dataset, epochs=10, steps_per_epoch=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#outcomes = torch.zeros((1, 2))\n",
    "#approximator.sample(batch_size, data = {\"outcomes\": Tensor([0]), \"designs\": Tensor([0])})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO at last\n",
    "\n",
    "class InferenceDesignApproximator:\n",
    "    def __init__(self, hyperparameters: dict, bf_settings: dict, design_settings: dict):\n",
    "\n",
    "        self.summary_network = ...\n",
    "\n",
    "        # Dataset object: online dataset https://github.com/stefanradev93/BayesFlow/blob/streamlined-backend/bayesflow/datasets/online_dataset.py\n",
    "        self.dataset: MyDataSet\n",
    "\n",
    "        # BayesFlow approximator is encapsuled\n",
    "        self.bf_approximator = bf.approximators.Approximator(..., summary_network, **bf_settings)\n",
    "        \n",
    "        # Design network object is encapsuled\n",
    "        self.design_net = DesignNetwork(..., summary_network, **design_settings)\n",
    "\n",
    "        # Hyperparameters: weight terms to balance losses, etc.\n",
    "        self.hyperparameters = hyperparameters\n",
    "\n",
    "    def train(self, dataset):\n",
    "        # Stage 1: Train Bayesflow, use random design\n",
    "        self.bf_approximator.train(dataset)\n",
    "        self.dataset.stage = 2\n",
    "\n",
    "        # Stage 2: Fix BayesFlow, train design network\n",
    "        self.bf_approximator.freeze_weights() # implement this\n",
    "        self.design_approximator.train(dataset)\n",
    "        self.dataset.stage = 3\n",
    "\n",
    "        # Stage 3: Joint training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepAdaptiveDesign(nn.Module):\n",
    "  def __init__(\n",
    "      self,\n",
    "      encoder_net: nn.Module | bf.networks.CouplingFlow,\n",
    "      decoder_net: nn.Module,\n",
    "      design_shape: torch.Size\n",
    "    ) -> None:\n",
    "    super().__init__()\n",
    "    self.design_shape = design_shape\n",
    "    # initialise first design with random normal\n",
    "    self.register_parameter(\n",
    "        \"initial_design\",\n",
    "        nn.Parameter(0.1 * torch.ones(design_shape, dtype=torch.float32))\n",
    "    )\n",
    "    self.encoder_net = encoder_net\n",
    "    self.decoder_net = decoder_net\n",
    "\n",
    "  def forward(self, designs=list[Tensor], outcomes=list[Tensor]) -> Tensor:\n",
    "    if len(outcomes) == 0:\n",
    "      return self.initial_design\n",
    "    else:\n",
    "      # embed design-outcome pairs\n",
    "      embeddings = torch.cat([self.encoder_net(xi, y) for (xi, y) in zip(designs, outcomes)]) # TODO aggregate [, dim]\n",
    "      # get next design\n",
    "      next_design = self.decoder_net(embeddings)\n",
    "    return next_design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmitterNetwork(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim,\n",
    "        hidden_dim,\n",
    "        output_dim,\n",
    "        n_hidden_layers=2,\n",
    "        activation=nn.Softplus,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.activation_layer = activation()\n",
    "        self.input_layer = nn.Linear(input_dim, hidden_dim)\n",
    "        if n_hidden_layers > 1:\n",
    "            self.middle = nn.Sequential(\n",
    "                *[\n",
    "                    nn.Sequential(nn.Linear(hidden_dim, hidden_dim), activation())\n",
    "                    for _ in range(n_hidden_layers - 1)\n",
    "                ]\n",
    "            )\n",
    "        else:\n",
    "            self.middle = nn.Identity()\n",
    "        self.output_layer = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, r):\n",
    "        x = self.input_layer(r)\n",
    "        x = self.activation_layer(x)\n",
    "        x = self.middle(x)\n",
    "        x = self.output_layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MutualInformation(nn.Module):\n",
    "  def __init__(self, joint_model, batch_size: int) -> None:\n",
    "    super().__init__()\n",
    "    self.joint_model = joint_model\n",
    "    self.batch_size = batch_size\n",
    "\n",
    "  def forward(self) -> Tensor:\n",
    "    raise NotImplemented\n",
    "\n",
    "  def estimate(num_eval_samples) -> float:\n",
    "    raise NotImplemented\n",
    "\n",
    "class NestedMonteCarlo(MutualInformation):\n",
    "  def __init__(\n",
    "      self,\n",
    "      joint_model: LikelihoodBasedModel,\n",
    "      amortized_posterior: bf.networks,\n",
    "      batch_size: torch.Size,\n",
    "      num_negative_samples: int,\n",
    "      lower_bound: bool = True\n",
    "      ) -> None:\n",
    "    super().__init__(joint_model=joint_model, batch_size=batch_size)\n",
    "    self.num_negative_samples = num_negative_samples # L\n",
    "    self.lower_bound = lower_bound\n",
    "\n",
    "  def forward(self) -> Tensor:\n",
    "\n",
    "    # simulate one trajectory of history (Batch instead?)\n",
    "    context_h, params_h, tau, xi_h, y_h = self.joint_model.sample(torch.Size([1])).values() \n",
    "\n",
    "    post_model_prob = self.joint_model.approximate_log_marginal_likelihood(context_h, params_h, xi_h, y_h)\n",
    "\n",
    "    context = np.random.choice(self.joint_model.context_sampler.possible_masks, \n",
    "                               size = self.batch_size, p = post_model_prob)\n",
    "    \n",
    "    prior_samples_primary = self.amortized_posterior.sample(context) # TODO\n",
    "\n",
    "    n_obs = self.joint_model.tau_sampler.max_obs - tau\n",
    "\n",
    "    _, _, _, designs, outcomes = self.joint_model.sample(self.batch_size, context = context, params = params, n_obs = n_obs)\n",
    "\n",
    "    # we can resuse negative samples\n",
    "    prior_samples_negative = self.amortized_posterior.sample(\n",
    "        torch.Size([self.num_negative_samples])\n",
    "    ).unsqueeze(1) # [num_neg_samples, ...] -> [num_neg_samples, 1, ...]\n",
    "\n",
    "    # evaluate the logprob of outcomes under the primary:\n",
    "    logprob_primary = torch.stack([\n",
    "        self.joint_model.outcome_likelihood(\n",
    "            prior_samples_primary, xi\n",
    "        ).log_prob(y) for (xi, y) in zip(designs, outcomes)\n",
    "    ], dim=0).sum(0) # [T, B] -> [B]\n",
    "\n",
    "    # evaluate the logprob of outcomes under the contrastive parameter samples:\n",
    "    logprob_negative = torch.stack([\n",
    "        self.joint_model.outcome_likelihood(\n",
    "            prior_samples_negative, xi.unsqueeze(0) # add dim for <num_neg_samples>\n",
    "        ).log_prob(y.unsqueeze(0)) for (xi, y) in zip(designs, outcomes)\n",
    "    ], dim=0).sum(0) # [T, num_neg_samples, B] -> [num_neg_samples, B]\n",
    "\n",
    "    print(\"nagative param\", prior_samples_negative.shape)\n",
    "    print(\"one design\", designs[0].unsqueeze(0).shape)\n",
    "    print(\"one outcomes\", outcomes[0].unsqueeze(0).shape)\n",
    "\n",
    "    # if lower bound, log_prob primary should be added to the denominator\n",
    "    if self.lower_bound:\n",
    "      # concat primary and negative to get [negative_b + 1, B] for the logsumexp\n",
    "      logprob_negative = torch.cat([\n",
    "          logprob_negative, logprob_primary.unsqueeze(0)]\n",
    "      ) # [num_neg_samples + 1, B]\n",
    "      to_logmeanexp = torch.log(self.num_negative_samples + 1)\n",
    "    else:\n",
    "      to_logmeanexp = torch.log(self.num_negative_samples)\n",
    "\n",
    "    log_denom = torch.logsumexp(logprob_negative, dim=0) - to_logmeanexp # [B]\n",
    "    mi = (logprob_primary - log_denom).mean(0) # [B] -> scalar\n",
    "    return -mi\n",
    "\n",
    "  def estimate(self) -> float:\n",
    "    with torch.no_grad():\n",
    "      loss = self.forward()\n",
    "    return -loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bayesflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
