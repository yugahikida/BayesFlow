{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"KERAS_BACKEND\"] = \"torch\"\n",
    "os.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"1\"\n",
    "\n",
    "import keras\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "# import bayesflow as bf\n",
    "\n",
    "import torch\n",
    "from torch import Tensor\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After loading bayesflow, the computational graph collapses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepAdaptiveDesign(nn.Module):\n",
    "  def __init__(\n",
    "      self,\n",
    "      # encoder_net: nn.Module, # same summary for bf and dad or different?\n",
    "      decoder_net: nn.Module,\n",
    "      design_shape: torch.Size, # [xi_dim]\n",
    "      summary_variables: list[str] = None # in case of using summary_net from bf\n",
    "    ) -> None:\n",
    "    super().__init__()\n",
    "    self.design_shape = design_shape\n",
    "    self.register_parameter(\n",
    "        \"initial_design\",\n",
    "        nn.Parameter(0.1 * torch.ones(design_shape, dtype=torch.float32)) # scalar\n",
    "    )\n",
    "    # self.encoder_net = encoder_net\n",
    "    self.decoder_net = decoder_net\n",
    "    self.summary_variables = summary_variables\n",
    "\n",
    "  def forward(self, history, batch_size: int) -> Tensor:\n",
    "    if history is None:\n",
    "      return self.initial_design\n",
    "    else:\n",
    "      # embed design-outcome pairs\n",
    "      # embeddings = self.encoder_net(filter_concatenate(history, keys=self.summary_variables)).to('cpu').requires_grad_(True)  # in case of using summary_net from bf. [B, summary_dim]\n",
    "      embeddings = torch.rand([1, 10])\n",
    "      # get next design\n",
    "      next_design = self.decoder_net(embeddings)\n",
    "    return next_design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmitterNetwork(nn.Module):\n",
    "  def __init__(\n",
    "        self,\n",
    "        input_dim, # summary_dim\n",
    "        hidden_dim,\n",
    "        output_dim, # xi_dim\n",
    "        n_hidden_layers=2,\n",
    "        activation=nn.Softplus,\n",
    "    ):\n",
    "    super().__init__()\n",
    "    self.activation_layer = activation()\n",
    "    self.input_layer = nn.Linear(input_dim, hidden_dim)\n",
    "    if n_hidden_layers > 1:\n",
    "      self.middle = nn.Sequential(\n",
    "         *[\n",
    "            nn.Sequential(nn.Linear(hidden_dim, hidden_dim), activation())\n",
    "            for _ in range(n_hidden_layers - 1)\n",
    "          ]\n",
    "            )\n",
    "    else:\n",
    "      self.middle = nn.Identity()\n",
    "      \n",
    "    self.output_layer = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "  def forward(self, r):\n",
    "    print(\"Input requires grad:\", r.requires_grad) \n",
    "    x = self.input_layer(r)\n",
    "    print(\"Input requires grad:\", x.requires_grad) \n",
    "    x = self.activation_layer(x)\n",
    "    print(\"Input requires grad:\", x.requires_grad) \n",
    "    x = self.middle(x)\n",
    "    print(\"Input requires grad:\", x.requires_grad) \n",
    "    x = self.output_layer(x)\n",
    "    return x.unsqueeze(1) # [B, xi_dim] -> [B, 1, xi_dim]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Simulator(nn.Module):\n",
    "    def __init__(self, design_net):\n",
    "        super().__init__()\n",
    "        self.design_net = design_net\n",
    "    \n",
    "    def forward(self):\n",
    "        designs = []\n",
    "        \n",
    "        for i in range(5):\n",
    "            history = None if i == 0 else 0\n",
    "\n",
    "            xi = self.design_net(history, 0)\n",
    "\n",
    "            if history is None:\n",
    "                xi = xi.expand(1, 1, 1) # for initial design\n",
    "\n",
    "            designs.append(xi)\n",
    "\n",
    "        designs = torch.cat(designs, dim=0)\n",
    "\n",
    "        return designs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = EmitterNetwork(10, 24, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "design_net = DeepAdaptiveDesign(\n",
    "    decoder_net=decoder,\n",
    "    design_shape=torch.Size([1])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = torch.rand([1, 10], requires_grad=True) # embedding (just for testing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we would expect True to be printed four times -> computational graph is built properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input requires grad: True\n",
      "Input requires grad: True\n",
      "Input requires grad: True\n",
      "Input requires grad: True\n"
     ]
    }
   ],
   "source": [
    "output = decoder(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For initail design, we expect None for `grad_fn` since `initial_design` is a leaf node.\n",
    "\n",
    "But still `requires_grad = True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "next_design = design_net(history = None, batch_size  = 0) # we didn't need batch_size arg actually..\n",
    "print(next_design.grad_fn)\n",
    "print(next_design.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we input *other than* `None` in `history`, this implements design given some data hence we should have `grad_fn` available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input requires grad: False\n",
      "Input requires grad: True\n",
      "Input requires grad: True\n",
      "Input requires grad: True\n",
      "<UnsqueezeBackward0 object at 0x10bf29750>\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "next_design = design_net(history = 0, batch_size  = 0) \n",
    "print(next_design.grad_fn)\n",
    "print(next_design.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "simulator = Simulator(design_net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input requires grad: False\n",
      "Input requires grad: True\n",
      "Input requires grad: True\n",
      "Input requires grad: True\n",
      "Input requires grad: False\n",
      "Input requires grad: True\n",
      "Input requires grad: True\n",
      "Input requires grad: True\n",
      "Input requires grad: False\n",
      "Input requires grad: True\n",
      "Input requires grad: True\n",
      "Input requires grad: True\n",
      "Input requires grad: False\n",
      "Input requires grad: True\n",
      "Input requires grad: True\n",
      "Input requires grad: True\n",
      "<CatBackward0 object at 0x10bf2a530>\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "out = simulator()\n",
    "print(out.grad_fn)\n",
    "print(out.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`requires_grad = True` automatically except for input. \n",
    "It is a natural bahavior of pytorch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bayesflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
